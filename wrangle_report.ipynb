{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reporting: wragle_report\n",
    "\n",
    "I initiated the data wrangling procedure by collecting the required data for analysis. The initial step involved importing the Twitter archive data, which had been directly acquired from a Twitter user and sent to us via email. This data was then transformed into a structured data frame. Additionally, I employed the requests library to retrieve image prediction data, which was subsequently organized into a data frame. To handle the JSON data utilized for compiling retweet and favorite counts, I read the file line by line, creating a dedicated data frame for this purpose.\n",
    "\n",
    "Subsequently, I began the process of evaluating the data through both visual inspection and programmatic methods. Visually, I examined each data set using the `.head()` function, successfully identifying anomalies such as the presence of '+0000' appended to the data in the 'timestamp' column within the archive data frame. Furthermore, I detected inconsistencies in the labeling of dog breeds within the image data frame.\n",
    "\n",
    "Programmatically, I employed methods such as `.info()`, `.value_counts()`, `.sum()`, `.describe()`, and `.duplicated()` to scrutinize the data frames. This comprehensive examination enabled the identification of various issues requiring attention, including:\n",
    "- Incorrect data type for the 'timestamp' column.\n",
    "- Necessity to eliminate all retweets from the data frame.\n",
    "- Need to convert 'in_reply_to_status_id' and 'in_reply_to_user_id' to integer data types.\n",
    "- Validation required for denominators exceeding 10.\n",
    "- Verification needed for numerator ratings displaying odd value counts.\n",
    "- Notable observation: Certain dog breed predictions were not accurate.\n",
    "\n",
    "Once these issues were pinpointed, I began the data cleaning process and ensured the creation of copies of the original data sets. The initial step in this cleaning endeavor involved merging retweet counts and favorite counts from the 'tweet_json' data frame into the primary archive data frame. This action led to changes in the data types of these values, necessitating corrections during the cleaning phase. Simultaneously, I streamlined the image prediction data frame, retaining only the tweet ID and the dog breed with the highest confidence for each image. Instances where no dog breed was identified were duly noted. I subsequently integrated this processed data into the main archive data frame, enhancing the efficiency of future cleaning and analysis steps.\n",
    "\n",
    "Finally, I addressed any remaining issues identified during the assessment phase, such as rectifying the incorrect data type in the 'timestamp' column. During the cleaning process, I also opted to discard certain columns that were deemed unnecessary, thus contributing to improved efficiency. Following these steps, I proceeded to conduct a succinct analysis and visualization of the data, the outcomes of which are documented in the 'act_report.html' file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
